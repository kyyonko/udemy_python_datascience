{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ナイーブベイズ分類"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Part 1: 数学的な準備\n",
    "Part 2: ベイズの定理\n",
    "Part 3: ナイーブベイズの紹介\n",
    "Part 4: 数学的な背景\n",
    "Part 5: 確率を使った分類\n",
    "Part 6: Gaussian Naive Bayes\n",
    "Part 7: Scikit Learnを使ったGaussian Naive Bayes\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## Part 1: 数学的な準備、Part 2: ベイズの定理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "省略。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## Part 3: ナイーブベイズの紹介"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ナイーブベイズ（Naive Bayes）は、スパムメールの分類などに実際に利用されている機械学習アルゴリズム。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## Part 4: 数学的な背景"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "yが目的変数、説明変数が x<sub>1</sub> から x<sub>n</sub> まであるとする。\n",
    "ベイズの定理を使うと、与えられた説明変数を元に、そのサンプルがどのクラスに属するかの確率を次のような式で計算できる。\n",
    "\n",
    "$$P(y \\mid x_1, \\dots, x_n) = \\frac{P(y) P(x_1, \\dots x_n \\mid y)}\n",
    "                                 {P(x_1, \\dots, x_n)}$$\n",
    "                                 \n",
    "ナイーブベイズのナイーブは、各説明変数が互いに独立であるという仮定から来ている。\n",
    "$$P(x_i | y, x_1, \\dots, x_{i-1}, x_{i+1}, \\dots, x_n) = P(x_i | y)$$\n",
    "\n",
    "この仮定のもとでは、すべての i について、式を次のように変形できる。\n",
    "\n",
    "$$P(y \\mid x_1, \\dots, x_n) = \\frac{P(y) \\prod_{i=1}^{n} P(x_i \\mid y)}\n",
    "                                 {P(x_1, \\dots, x_n)}$$\n",
    "                               \n",
    "それぞれの変数について、クラスごとの確率を求めればよいので、計算が楽になる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## Part 5: 確率を使った分類\n",
    "\n",
    "ナイーブベイズではそれぞれのクラスに属する確率が計算されるので、\n",
    "最終的にはそのサンプルを、確率が最も大きいクラスに分類する。ここで、arg maxの記号が出てくる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P(x<sub>1</sub>, ..., x<sub>n</sub>) は手元のデータセットに関しては一定の値なので、無視できる。\n",
    "\n",
    "$$P(y \\mid x_1, \\dots, x_n) \\propto P(y) \\prod_{i=1}^{n} P(x_i \\mid y)$$\n",
    "\n",
    "最終的には、もっとも大きな確率が割り当たるクラスに、サンプルを分類する。\n",
    "\n",
    "$$\\hat{y} = \\arg\\max_y P(y) \\prod_{i=1}^{n} P(x_i \\mid y),$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## Part 6: Gaussian Naive Bayes\n",
    "\n",
    "説明変数が連続値の場合、これを正規分布に従うものとしてモデル化すると、モデルの構築や計算が楽に。サンプルデータのアヤメのデータも連続値ですので、後ほどの、Gaussian Naive Bayesを利用する。\n",
    "\n",
    "$$p(x=v|c)=\\frac{1}{\\sqrt{2\\pi\\sigma^2_c}}\\,e^{ -\\frac{(v-\\mu_c)^2}{2\\sigma^2_c} }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## Part 7: Scikit learnを使ったGaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import Series,DataFrame\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Gaussian Naive Bayes のためのコード\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# またirisデータを使う\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data\n",
    "\n",
    "Y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# インスタンス作成\n",
    "model = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# データをテスト用とトレーニング用に分割\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# あてはめ\n",
    "model.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# 予測の精度を比較\n",
    "predicted = model.predict(X_test)\n",
    "expected = Y_test\n",
    "print(metrics.accuracy_score(expected, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
